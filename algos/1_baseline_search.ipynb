{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T20:33:32.090096Z",
     "iopub.status.busy": "2025-11-29T20:33:32.089928Z",
     "iopub.status.idle": "2025-11-29T20:33:44.563939Z",
     "shell.execute_reply": "2025-11-29T20:33:44.563108Z",
     "shell.execute_reply.started": "2025-11-29T20:33:32.090080Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Install evaluate\n",
    "!pip install -q evaluate\n",
    "\n",
    "# 2. THE FIX: Force install this specific version of protobuf\n",
    "!pip install -q \"protobuf==3.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:33:44.565841Z",
     "iopub.status.busy": "2025-11-29T20:33:44.565632Z",
     "iopub.status.idle": "2025-11-29T20:34:13.221929Z",
     "shell.execute_reply": "2025-11-29T20:34:13.221342Z",
     "shell.execute_reply.started": "2025-11-29T20:33:44.565817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict, IterableDataset, IterableDatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EvalPrediction\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.223230Z",
     "iopub.status.busy": "2025-11-29T20:34:13.222664Z",
     "iopub.status.idle": "2025-11-29T20:34:13.227006Z",
     "shell.execute_reply": "2025-11-29T20:34:13.226316Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.223210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import the Python types\n",
    "from typing import List, Dict, Any, Tuple, cast, Optional\n",
    "\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.227970Z",
     "iopub.status.busy": "2025-11-29T20:34:13.227759Z",
     "iopub.status.idle": "2025-11-29T20:34:13.247704Z",
     "shell.execute_reply": "2025-11-29T20:34:13.247019Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.227955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_SAMPLE_SIZE = 3000\n",
    "TOTAL_TRIALS = 20\n",
    "NUM_LABELS = 6\n",
    "MAX_LENGTH = 128\n",
    "MODEL = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.248667Z",
     "iopub.status.busy": "2025-11-29T20:34:13.248411Z",
     "iopub.status.idle": "2025-11-29T20:34:13.262562Z",
     "shell.execute_reply": "2025-11-29T20:34:13.261827Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.248646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int):\n",
    "  \"\"\"\n",
    "  Set the global seed for reproducibility.\n",
    "  \"\"\"\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  # Check if CUDA GPU is available\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.263960Z",
     "iopub.status.busy": "2025-11-29T20:34:13.263383Z",
     "iopub.status.idle": "2025-11-29T20:34:13.281467Z",
     "shell.execute_reply": "2025-11-29T20:34:13.280882Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.263934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.283947Z",
     "iopub.status.busy": "2025-11-29T20:34:13.283685Z",
     "iopub.status.idle": "2025-11-29T20:34:13.290834Z",
     "shell.execute_reply": "2025-11-29T20:34:13.290167Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.283930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True, order=True)\n",
    "class LoraHyperparameters:\n",
    "  learning_rate: float\n",
    "  warmup_ratio: float\n",
    "  rank: int\n",
    "  alpha: int\n",
    "  dropout: float\n",
    "  target_modules: List[str]\n",
    "\n",
    "  @staticmethod # The 'generate_random_hyperparameters' doesn't take an instance of 'self', hence why we use '@staticmethod'\n",
    "  def generate_random_hyperparameters() -> 'LoraHyperparameters':\n",
    "\n",
    "    # Target modules: (Attention Only) OR (Attention + Feedforward)\n",
    "      # Option A: [\"q_lin\", \"v_lin\"]\n",
    "      # Option B: [\"q_lin\", \"v_lin\", \"ffn.lin1\", \"ffn.lin2\"]\n",
    "    module_choice = random.choice([\"attn\", \"attn_ffn\"])\n",
    "\n",
    "    if module_choice == \"attn\":\n",
    "        target_modules = [\"q_lin\", \"v_lin\"]\n",
    "    else:\n",
    "        target_modules = [\"q_lin\", \"v_lin\", \"ffn.lin1\", \"ffn.lin2\"]\n",
    "\n",
    "    # Return an instance of the LoraHyperparameters class\n",
    "    return LoraHyperparameters(\n",
    "      # learning_rate=random.uniform(5e-6, 5e-4), # Learning rate is a continous value\n",
    "      learning_rate=random.uniform(1e-5, 2e-4),\n",
    "      warmup_ratio=random.choice([0.0, 0.06, 0.1]), # Warm-up ratio is a discrete value\n",
    "      rank=random.choice([2, 4, 8, 16, 24]), # LoRA rank is a continous value\n",
    "      alpha = random.choice([8, 16, 32, 64, 96]), # Alpha is a discrete value\n",
    "      dropout = random.choice([0, 0.05, 0.1, 0.2]), # Dropout is a discrete value\n",
    "      target_modules=target_modules\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.291582Z",
     "iopub.status.busy": "2025-11-29T20:34:13.291400Z",
     "iopub.status.idle": "2025-11-29T20:34:13.308751Z",
     "shell.execute_reply": "2025-11-29T20:34:13.308160Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.291567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "  def __init__(self, model_name: str = MODEL):\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    self.dataset: Optional[Dict[str, Any]] = None\n",
    "\n",
    "  def prepare_data(self) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads the dataset and processes it.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the dataset is correctly loaded into the instance memory\n",
    "    if self.dataset is not None:\n",
    "        return self.dataset\n",
    "\n",
    "    print(\"Loading and processing data...\")\n",
    "\n",
    "    # Load full dataset\n",
    "    full_dataset = cast(DatasetDict, load_dataset(\"dair-ai/emotion\"))\n",
    "\n",
    "    # Use seed to ensure every run uses the SAME subset of data\n",
    "    train_subset = full_dataset[\"train\"].shuffle(seed=SEED).select(range(TRAIN_SAMPLE_SIZE))\n",
    "\n",
    "    # Private helper method for text embeddings\n",
    "    def _tokenize(examples):\n",
    "      return self.tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH\n",
    "      )\n",
    "\n",
    "    tokenized_train_dataset = train_subset.map(_tokenize, batched=True)\n",
    "    tokenized_validation_dataset = full_dataset[\"validation\"].map(_tokenize, batched=True)\n",
    "\n",
    "    self.dataset = {\n",
    "        \"train\": tokenized_train_dataset,\n",
    "        \"validation\": tokenized_validation_dataset,\n",
    "        \"tokenizer\": self.tokenizer,\n",
    "        \"num_labels\": NUM_LABELS\n",
    "    }\n",
    "\n",
    "    print(\"Data preparation complete.\")\n",
    "\n",
    "    return self.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:13.309525Z",
     "iopub.status.busy": "2025-11-29T20:34:13.309334Z",
     "iopub.status.idle": "2025-11-29T20:34:22.020950Z",
     "shell.execute_reply": "2025-11-29T20:34:22.020121Z",
     "shell.execute_reply.started": "2025-11-29T20:34:13.309503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_manager = DataManager()\n",
    "data_bundle = data_manager.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:22.022110Z",
     "iopub.status.busy": "2025-11-29T20:34:22.021882Z",
     "iopub.status.idle": "2025-11-29T20:34:22.041029Z",
     "shell.execute_reply": "2025-11-29T20:34:22.040332Z",
     "shell.execute_reply.started": "2025-11-29T20:34:22.022093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomSearchExperiment:\n",
    "  def __init__(self, data_bundle: Dict[str, Any], total_trials: int = 20):\n",
    "    self.data = data_bundle\n",
    "    self.total_trials = total_trials\n",
    "    self.results: List[Dict[str, Any]] = []\n",
    "    self.metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "  def _compute_metrics(self, eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates accuracy during training.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    result = self.metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return cast(Dict[str, float], result)\n",
    "\n",
    "  def _cleanup_memory(self, model, trainer):\n",
    "    \"\"\"\n",
    "    Forcefully clears GPU memory.\n",
    "    \"\"\"\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "  def run_single_trial(self, trial_id: int, params: LoraHyperparameters, seed: Optional[int] = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Executes one training run with specific hyperparameters.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Trial {trial_id}/{self.total_trials}] Starting...\")\n",
    "    print(f\"   Params: Rank={params.rank}, Alpha={params.alpha}, LR={params.learning_rate:.2e}\")\n",
    "\n",
    "    # # Add 'trial_id' to the 'SEED' to ensure each trial is unique\n",
    "    # current_seed = SEED + trial_id\n",
    "\n",
    "    if seed is not None:\n",
    "      current_seed = seed\n",
    "    else:\n",
    "      current_seed = SEED + trial_id\n",
    "\n",
    "    # Initialize the base model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "      MODEL,\n",
    "      num_labels=self.data[\"num_labels\"]\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "      task_type=TaskType.SEQ_CLS,\n",
    "      r=params.rank,\n",
    "      lora_alpha=params.alpha,\n",
    "      lora_dropout=params.dropout,\n",
    "      target_modules=params.target_modules\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "      output_dir=f\"./results/trial_{trial_id}\",\n",
    "      learning_rate=params.learning_rate,\n",
    "      per_device_train_batch_size=16,\n",
    "      per_device_eval_batch_size=16,\n",
    "      num_train_epochs=3,\n",
    "      warmup_ratio=params.warmup_ratio,\n",
    "      weight_decay=0.01,\n",
    "      eval_strategy=\"epoch\", # Updated from 'evaluation_strategy'\n",
    "      save_strategy=\"no\", # Don't save checkpoints (saves disk space)\n",
    "      logging_strategy=\"epoch\",\n",
    "      seed=current_seed,\n",
    "      report_to=\"none\", # Disable WANDB\n",
    "      load_best_model_at_end=False,\n",
    "      optim=\"adamw_torch\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=args,\n",
    "      train_dataset=self.data[\"train\"],\n",
    "      eval_dataset=self.data[\"validation\"],\n",
    "      data_collator=DataCollatorWithPadding(tokenizer=self.data[\"tokenizer\"]),\n",
    "      compute_metrics=self._compute_metrics\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train and Evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    end_time = time.time()\n",
    "    trial_duration = end_time - start_time\n",
    "\n",
    "    accuracy = eval_results[\"eval_accuracy\"]\n",
    "\n",
    "    print(f\"   [Trial {trial_id}] Complete. Accuracy: {accuracy:.4%} | Time: {trial_duration:.2f}s\")\n",
    "\n",
    "    # Cleanup the memory\n",
    "    self._cleanup_memory(model, trainer)\n",
    "\n",
    "    return accuracy, trial_duration\n",
    "\n",
    "  def verify_top_trials(self, top_k: int = 5, seeds: List[int] = [42, 43, 44]):\n",
    "    # Check if there are any results\n",
    "    if not self.results:\n",
    "      print(\"No results found in memory. Please run experiment first.\")\n",
    "      return\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"STARTING ROBUSTNESS VERIFICATION (Top {top_k} Models)\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Sort the results by accuracy in descending order and slice the top K\n",
    "    sorted_results = sorted(self.results, key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "    top_k_results = sorted_results[:top_k]\n",
    "\n",
    "    robustness_data: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, trial in enumerate(top_k_results, start=1):\n",
    "      trial_id = trial[\"trial_id\"]\n",
    "      original_accuracy = trial[\"accuracy\"] # Renamed to avoid confusion\n",
    "      \n",
    "      print(f\"\\n>>> Verifying Rank {i}: Trial {trial_id} (Original Acc: {original_accuracy:.4%})\")\n",
    "\n",
    "      params = LoraHyperparameters(\n",
    "          learning_rate=trial[\"learning_rate\"],\n",
    "          warmup_ratio=trial[\"warmup_ratio\"],\n",
    "          rank=trial[\"rank\"],\n",
    "          alpha=trial[\"alpha\"],\n",
    "          dropout=trial[\"dropout\"],\n",
    "          target_modules=trial[\"target_modules\"]\n",
    "      )\n",
    "\n",
    "      current_accuracies = []\n",
    "\n",
    "      for seed in seeds:\n",
    "        # Fixed Argument passing syntax here\n",
    "        new_acc, _ = self.run_single_trial(trial_id, params, seed=seed)\n",
    "        current_accuracies.append(new_acc)\n",
    "\n",
    "        # Explicit garbage collection\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "      \n",
    "      mean_accuracy = np.mean(current_accuracies)\n",
    "      std_accuracy = np.std(current_accuracies)\n",
    "      \n",
    "      print(f\"    -> Result: {mean_accuracy:.4%} Â± {std_accuracy:.4%}\")\n",
    "\n",
    "      entry = {\n",
    "          \"trial_id\": trial_id,\n",
    "          \"original_accuracy\": original_accuracy,\n",
    "          \"mean_accuracy\": mean_accuracy,\n",
    "          \"std_accuracy\": std_accuracy,\n",
    "          \"all_seed_accuracies\": current_accuracies\n",
    "      }\n",
    "\n",
    "      entry.update(asdict(params))\n",
    "      robustness_data.append(entry)\n",
    "\n",
    "    df_robust = pd.DataFrame(robustness_data)\n",
    "    filename = \"robustness_verification_results.csv\"\n",
    "    df_robust.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"\\nRobustness verification complete. Saved to {filename}\")\n",
    "\n",
    "\n",
    "  def run_experiment(self):\n",
    "    \"\"\"\n",
    "    Main loop to execute the random search.\n",
    "    \"\"\"\n",
    "    print(f\"Starting Random Search for {self.total_trials} trials...\")\n",
    "\n",
    "    for i in range(self.total_trials):\n",
    "      trial_id = i + 1\n",
    "\n",
    "      try:\n",
    "        # Generate the random hyperparameters\n",
    "        params = LoraHyperparameters.generate_random_hyperparameters()\n",
    "\n",
    "        # Run the current trial\n",
    "        accuracy, trial_duration = self.run_single_trial(trial_id=trial_id, params=params)\n",
    "\n",
    "        # Log the current result\n",
    "        result_entry = {\n",
    "            \"trial_id\": trial_id,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"trial_duration_in_seconds\": trial_duration\n",
    "        }\n",
    "\n",
    "        # Flatten parameters into the dict for easier CSV saving\n",
    "        result_entry.update(asdict(params))\n",
    "\n",
    "        self.results.append(result_entry)\n",
    "\n",
    "      except Exception as e:\n",
    "        print(f\"!!! CRITICAL ERROR in Trial {trial_id}: {e}\")\n",
    "\n",
    "        # Clean the memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nExperiment Completed.\")\n",
    "\n",
    "  def save_results(self, filename=\"random_search_results.csv\"):\n",
    "    \"\"\"\n",
    "    Saves results to CSV and prints summary stats.\n",
    "    \"\"\"\n",
    "    if not self.results:\n",
    "      print(\"No results to save.\")\n",
    "      return\n",
    "\n",
    "    df = pd.DataFrame(self.results)\n",
    "\n",
    "    # Summary\n",
    "    best_idx = df['accuracy'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "\n",
    "    best_accuracy = best_row['accuracy'].item()\n",
    "    best_trial_id = int(best_row['trial_id'].item())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4%} (Trial {best_trial_id})\")\n",
    "    print(f\"Mean Accuracy: {df['accuracy'].mean():.4%}\")\n",
    "    print(f\"Std Dev      : {df['accuracy'].std():.4%}\")\n",
    "\n",
    "    # Export\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T20:34:22.042166Z",
     "iopub.status.busy": "2025-11-29T20:34:22.041825Z",
     "iopub.status.idle": "2025-11-29T21:11:38.141621Z",
     "shell.execute_reply": "2025-11-29T21:11:38.140926Z",
     "shell.execute_reply.started": "2025-11-29T20:34:22.042140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the Experiment\n",
    "experiment = RandomSearchExperiment(data_bundle, total_trials=TOTAL_TRIALS)\n",
    "\n",
    "start_experiment_time = time.time()\n",
    "\n",
    "# Run the full experiment of 20 trials\n",
    "experiment.run_experiment()\n",
    "\n",
    "end_experiment_time = time.time()\n",
    "\n",
    "total_duration = end_experiment_time - start_experiment_time\n",
    "\n",
    "print(f\"Total time taken to complete 20 experiments: {str(total_duration)} seconds\")\n",
    "\n",
    "# Save the results from running the full experiment\n",
    "experiment.save_results()\n",
    "\n",
    "experiment.verify_top_trials(top_k=5, seeds=[42, 43, 44])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
